{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import  Input,BatchNormalization,Conv2D,MaxPool2D,Flatten,Dense,Activation,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e83042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path = \"../data/Training/\"\n",
    "testing_data_path =  \"../data/Validation/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa3b63e",
   "metadata": {},
   "source": [
    "#### Total number of classes and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = sorted(os.listdir(training_data_path))\n",
    "\n",
    "number_of_male_images = len(os.listdir(training_data_path+\"male\"))\n",
    "number_of_female_images = len(os.listdir(training_data_path+\"female\"))\n",
    "\n",
    "print(\"Total Number of Classes :\",len(class_names))\n",
    "print(\"Classes Names :\",class_names)\n",
    "print(\"Total Male images :\",number_of_male_images)\n",
    "print(\"Total Female images :\",number_of_female_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b7e5a2",
   "metadata": {},
   "source": [
    "#### Visualizing Distribution of both genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b00c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(np.array([number_of_female_images,number_of_male_images]),labels=class_names,autopct='%1.1f%%')\n",
    "plt.title('Gender Distribution')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bc0934",
   "metadata": {},
   "source": [
    "Checking distribution of Test data also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a047446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_male_images_valid = len(os.listdir(testing_data_path+\"male\"))\n",
    "number_of_female_images_valid = len(os.listdir(testing_data_path+\"female\"))\n",
    "plt.pie(np.array([number_of_female_images_valid,number_of_male_images_valid]),labels=class_names,autopct='%1.1f%%')\n",
    "plt.title('Gender Distribution')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a57be16",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f29ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_images = []\n",
    "train_labels = []\n",
    "i = 0\n",
    "for folder_name in class_names:\n",
    "    folder_path = os.path.join(training_data_path, folder_name)\n",
    "    i=0\n",
    "    for img_path in os.listdir(folder_path):\n",
    "        print(i)\n",
    "        i=i+1\n",
    "        img = cv2.imread(os.path.join(folder_path, img_path))\n",
    "        img = cv2.resize(img, (50, 50)) # resize image to (50, 50)\n",
    "        img = img/255.0\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        train_images.append(img)\n",
    "        if folder_name == \"male\":\n",
    "            train_labels.append(1) # set label to 1 if female\n",
    "        else:\n",
    "            train_labels.append(0) # set label to 0 if male\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "train_images = train_images.reshape(-1, 50, 50, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_images = []\n",
    "valid_labels = []\n",
    "\n",
    "for folder_name in class_names:\n",
    "    folder_path = os.path.join(testing_data_path, folder_name)\n",
    "    for img_path in os.listdir(folder_path):\n",
    "        img = cv2.imread(os.path.join(folder_path, img_path))\n",
    "        img = cv2.resize(img, (50, 50)) # resize image to (50, 50)\n",
    "        img = img/255.0\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        valid_images.append(img)\n",
    "        if folder_name == \"male\":\n",
    "            valid_labels.append(1) # set label to 1 if female\n",
    "        else:\n",
    "            valid_labels.append(0) # set label to 0 if male\n",
    "\n",
    "valid_images = np.array(valid_images)\n",
    "valid_labels = np.array(valid_labels)\n",
    "valid_images = valid_images.reshape(-1, 50, 50, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7a270c",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a572ee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape = (50, 50, 3))\n",
    "x = Conv2D(filters = 16 , kernel_size=(3, 3), padding='SAME', activation='relu', input_shape=(50, 50, 3))(inp)\n",
    "x = Conv2D(16 , (3,3), padding = \"same\", activation='relu')(x)\n",
    "x = Conv2D(16 , (3,3), padding = \"same\", activation='relu')(x)\n",
    "x = BatchNormalization(momentum=0.9)(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(16, (3,3), padding = \"same\", activation='relu')(x)\n",
    "x = Dropout(rate=0.3)(x)\n",
    "\n",
    "\n",
    "x = Conv2D(filters = 32 , kernel_size=(3, 3), padding='SAME', activation='relu')(x)\n",
    "x = Conv2D(32 , (3,3), padding = \"same\", activation='relu')(x)\n",
    "x = Conv2D(32 , (3,3), padding = \"same\", activation='relu')(x)\n",
    "x = BatchNormalization(momentum=0.5)(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(32, (3,3), padding = \"same\", activation='relu')(x)\n",
    "x = Dropout(rate=0.4)(x)\n",
    "\n",
    "x = Conv2D(filters = 64 , kernel_size=(3, 3), padding='SAME', activation='relu')(x)\n",
    "x = Conv2D(64 , (3,3), padding = \"same\", activation='relu')(x)\n",
    "x = Conv2D(64 , (3,3), padding = \"same\", activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(64, (3,3), padding = \"same\", activation='relu')(x)\n",
    "\n",
    "x = Conv2D(filters = 128 , kernel_size=(3, 3), padding='SAME', activation='relu')(x)\n",
    "x = Conv2D(128 , (3,3), padding = \"same\", activation='relu')(x)\n",
    "x = Conv2D(128 , (3,3), padding = \"same\", activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(128, (3,3), padding = \"same\", activation='relu')(x)\n",
    "x = Conv2D(filters = 128 , kernel_size=(3, 3), padding='SAME', activation='relu')(x)\n",
    "x = Conv2D(128 , (3,3), padding = \"same\", activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(256 , activation = 'relu')(x) \n",
    "x = Dense(128 , activation = 'relu')(x) \n",
    "x = Dense(32 , activation = 'relu')(x) \n",
    "out = Dense(1 , activation = 'sigmoid')(x)\n",
    "model = Model(inputs=inp, outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f86efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d5df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec31702a",
   "metadata": {},
   "source": [
    "#### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70213161",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [EarlyStopping(patience=3, restore_best_weights=True),\n",
    "       ModelCheckpoint(filepath = \"ModelCheckpoint.h5\",\n",
    "                       monitor= \"val_loss\",\n",
    "                       save_best_only = True)]\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "history = model.fit(train_images,train_labels,callbacks=cbs, epochs=2) #epochs=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "# val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "# plt.plot(acc,  label='Training acc')\n",
    "# plt.plot(val_acc,  label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(loss,  label='Training loss')\n",
    "# plt.plot( val_loss,  label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8fea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('../models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ecf783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('../models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb9ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "\n",
    "# Load the image from the root folder\n",
    "\n",
    "def predict_image(model, image):\n",
    "    # Load the image using OpenCV\n",
    "#     image = cv2.imread(image_path)\n",
    "    # Resize the image to the input shape of the model\n",
    "    image = cv2.resize(image, (50, 50))\n",
    "    # Convert the image to a numpy array and normalize its values to be between 0 and 1\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    # Add a batch dimension to the image\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    # Make the prediction using the model\n",
    "    prediction = model.predict(image)\n",
    "    # Return the predicted class (0 or 1)\n",
    "    return int(prediction[0, 0] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe5e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "\n",
    "# Create a file input button\n",
    "file_input = widgets.FileUpload()\n",
    "\n",
    "# Display the file input button\n",
    "display(file_input)\n",
    "\n",
    "# Define a function to handle the file selection\n",
    "def on_file_upload(change):\n",
    "    for name, file_info in file_input.value.items():\n",
    "        # Load the image using OpenCV\n",
    "        image = cv2.imdecode(np.frombuffer(file_info['content'], np.uint8), cv2.IMREAD_COLOR)\n",
    "        # Display the image\n",
    "#         cv2.imshow('Image', image)\n",
    "        if predict_image(model,image)<0.5:\n",
    "           print(\"Female\")\n",
    "        else: print(\"Male\")\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Attach the file selection function to the file input button\n",
    "file_input.observe(on_file_upload, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b49732f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
